library(caret)
## Warning: package 'caret' was built under R version 3.1.2
## Loading required package: lattice
## Loading required package: ggplot2
library(rpart)
library(rpart.plot)
## Warning: package 'rpart.plot' was built under R version 3.1.2
library(randomForest)
## Warning: package 'randomForest' was built under R version 3.1.2
## randomForest 4.6-10
## Type rfNews() to see new features/changes/bug fixes.
library(corrplot)
## Warning: package 'corrplot' was built under R version 3.1.2
trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainFile <- "./data/pml-training.csv"
testFile  <- "./data/pml-testing.csv"
if (!file.exists("./data")) {
  dir.create("./data")
}
if (!file.exists(trainFile)) {
  download.file(trainUrl, destfile=trainFile, method="curl")
}
if (!file.exists(testFile)) {
  download.file(testUrl, destfile=testFile, method="curl")
}
#After downloading the data from the data source, we can read the two csv files into two data frames.
trainRaw <- read.csv("./data/pml-training.csv")
testRaw <- read.csv("./data/pml-testing.csv")
dim(trainRaw)
## [1] 19622   160
dim(testRaw)
## [1]  20 160
#The training data set contains 19622 observations and 160 variables, while the testing data set contains 
#20 observations and 160 variables. The “classe” variable in the training set is the outcome to predict.
sum(complete.cases(trainRaw))
## [1] 406
#First, we remove columns that contain NA missing values.
trainRaw <- trainRaw[, colSums(is.na(trainRaw)) == 0] 
testRaw <- testRaw[, colSums(is.na(testRaw)) == 0] 
#Next, we get rid of some columns that do not contribute much to the accelerometer measurements.
classe <- trainRaw$classe
trainRemove <- grepl("^X|timestamp|window", names(trainRaw))
trainRaw <- trainRaw[, !trainRemove]
trainCleaned <- trainRaw[, sapply(trainRaw, is.numeric)]
trainCleaned$classe <- classe
testRemove <- grepl("^X|timestamp|window", names(testRaw))
testRaw <- testRaw[, !testRemove]
testCleaned <- testRaw[, sapply(testRaw, is.numeric)]
#Now, the cleaned training data set contains 19622 observations and 53 variables, while the testing data set 
#contains 20 observations and 53 variables. The “classe” variable is still in the cleaned training set.
#Slice the data
#Then, we can split the cleaned training set into a pure training data set (70%) and a validation data set (30%). 
#We will use the validation data set to conduct cross validation in future steps.
#Data Modeling

#We fit a predictive model for activity recognition using Random Forest algorithm because it automatically selects 
#important variables and is robust to correlated covariates & outliers in general. We will use 5-fold cross validation 
#when applying the algorithm.
controlRf <- trainControl(method="cv", 5)
modelRf <- train(classe ~ ., data=trainData, method="rf", trControl=controlRf, ntree=250)
modelRf
## Random Forest 
## 
## 13737 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## 
## Summary of sample sizes: 10990, 10989, 10991, 10990, 10988 
## 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
##    2    0.9912642  0.9889483  0.001124489  0.001422736
##   27    0.9908999  0.9884874  0.002430602  0.003076372
##   52    0.9838389  0.9795534  0.001201645  0.001519375
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 2.
#Then, we estimate the performance of the model on the validation data set.
predictRf <- predict(modelRf, testData)
confusionMatrix(testData$classe, predictRf)
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1672    2    0    0    0
##          B    3 1131    5    0    0
##          C    0    5 1017    4    0
##          D    0    0   12  951    1
##          E    0    0    0    2 1080
## 
## Overall Statistics
##                                          
##                Accuracy : 0.9942         
##                  95% CI : (0.9919, 0.996)
##     No Information Rate : 0.2846         
##     P-Value [Acc > NIR] : < 2.2e-16      
##                                          
##                   Kappa : 0.9927         
##  Mcnemar's Test P-Value : NA             
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9982   0.9938   0.9836   0.9937   0.9991
## Specificity            0.9995   0.9983   0.9981   0.9974   0.9996
## Pos Pred Value         0.9988   0.9930   0.9912   0.9865   0.9982
## Neg Pred Value         0.9993   0.9985   0.9965   0.9988   0.9998
## Prevalence             0.2846   0.1934   0.1757   0.1626   0.1837
## Detection Rate         0.2841   0.1922   0.1728   0.1616   0.1835
## Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
## Balanced Accuracy      0.9989   0.9961   0.9909   0.9955   0.9993
accuracy <- postResample(predictRf, testData$classe)
accuracy
##  Accuracy     Kappa 
## 0.9942226 0.9926921
oose <- 1 - as.numeric(confusionMatrix(testData$classe, predictRf)$overall[1])
oose
## [1] 0.0057774
#So, the estimated accuracy of the model is 99.42% and the estimated out-of-sample error is 0.58%
#Predicting for Test Data Set
#Now, we apply the model to the original testing data set downloaded from the data source. We remove the problem_id 
#column first.
result <- predict(modelRf, testCleaned[, -length(names(testCleaned))])
result
##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E
#Appendix: Figures
#1.Correlation Matrix Visualization
corrPlot <- cor(trainData[, -length(names(trainData))])
corrplot(corrPlot, method="color")
#Tree Visualization
treeModel <- rpart(classe ~ ., data=trainData, method="class")
prp(treeModel) # fast plot





